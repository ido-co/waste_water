{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6935d142",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2c58fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"/home/yandex/MLW2022/idoc/detectron2/detectron2/\")\n",
    "import detectron2\n",
    "from detectron2.modeling import ROI_HEADS_REGISTRY\n",
    "from detectron2.modeling.roi_heads import Res5ROIHeads, select_foreground_proposals\n",
    "from detectron2.utils.logger import setup_logger\n",
    "l = setup_logger(output=\"./my_log_file.log\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "import os\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "\n",
    "from utils import MyTrainer\n",
    "\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from pycocotools.cocoeval import COCOeval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd0128f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_COCOeval(COCOeval):\n",
    "    def summarize(self):\n",
    "        '''\n",
    "        Compute and display summary metrics for evaluation results.\n",
    "        Note this functin can *only* be applied on the default parameter setting\n",
    "        '''\n",
    "        def _summarize( ap=1, iouThr=None, areaRng='all', maxDets=100 ):\n",
    "            p = self.params\n",
    "            iStr = ' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}'\n",
    "            titleStr = 'Average Precision' if ap == 1 else 'Average Recall'\n",
    "            typeStr = '(AP)' if ap==1 else '(AR)'\n",
    "            iouStr = '{:0.2f}:{:0.2f}'.format(p.iouThrs[0], p.iouThrs[-1]) \\\n",
    "                if iouThr is None else '{:0.2f}'.format(iouThr)\n",
    "\n",
    "            aind = [i for i, aRng in enumerate(p.areaRngLbl) if aRng == areaRng]\n",
    "            mind = [i for i, mDet in enumerate(p.maxDets) if mDet == maxDets]\n",
    "            if ap == 1:\n",
    "                # dimension of precision: [TxRxKxAxM]\n",
    "                s = self.eval['precision']\n",
    "                # IoU\n",
    "                if iouThr is not None:\n",
    "                    t = np.where(iouThr == p.iouThrs)[0]\n",
    "                    s = s[t]\n",
    "                s = s[:,:,:,aind,mind]\n",
    "            else:\n",
    "                # dimension of recall: [TxKxAxM]\n",
    "                s = self.eval['recall']\n",
    "                if iouThr is not None:\n",
    "                    t = np.where(iouThr == p.iouThrs)[0]\n",
    "                    s = s[t]\n",
    "                s = s[:,:,aind,mind]\n",
    "            if len(s[s>-1])==0:\n",
    "                mean_s = -1\n",
    "            else:\n",
    "                mean_s = np.mean(s[s>-1])\n",
    "\n",
    "                #cacluate AP(average precision) for each category\n",
    "                num_classes = 80\n",
    "                avg_ap = 0.0\n",
    "                if ap == 1:\n",
    "                    for i in range(0, num_classes):\n",
    "                        print('category : {0} : {1}'.format(i,np.mean(s[:,:,i,:])))\n",
    "                        avg_ap +=np.mean(s[:,:,i,:])\n",
    "                    print('(all categories) mAP : {}'.format(avg_ap / num_classes))\n",
    "\n",
    "            print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))\n",
    "            return mean_s\n",
    "        def _summarizeDets():\n",
    "            stats = np.zeros((12,))\n",
    "            stats[0] = _summarize(1)\n",
    "            stats[1] = _summarize(1, iouThr=.5, maxDets=self.params.maxDets[2])\n",
    "            stats[2] = _summarize(1, iouThr=.75, maxDets=self.params.maxDets[2])\n",
    "            stats[3] = _summarize(1, areaRng='small', maxDets=self.params.maxDets[2])\n",
    "            stats[4] = _summarize(1, areaRng='medium', maxDets=self.params.maxDets[2])\n",
    "            stats[5] = _summarize(1, areaRng='large', maxDets=self.params.maxDets[2])\n",
    "            stats[6] = _summarize(0, maxDets=self.params.maxDets[0])\n",
    "            stats[7] = _summarize(0, maxDets=self.params.maxDets[1])\n",
    "            stats[8] = _summarize(0, maxDets=self.params.maxDets[2])\n",
    "            stats[9] = _summarize(0, areaRng='small', maxDets=self.params.maxDets[2])\n",
    "            stats[10] = _summarize(0, areaRng='medium', maxDets=self.params.maxDets[2])\n",
    "            stats[11] = _summarize(0, areaRng='large', maxDets=self.params.maxDets[2])\n",
    "            return stats\n",
    "        def _summarizeKps():\n",
    "            stats = np.zeros((10,))\n",
    "            stats[0] = _summarize(1, maxDets=20)\n",
    "            stats[1] = _summarize(1, maxDets=20, iouThr=.5)\n",
    "            stats[2] = _summarize(1, maxDets=20, iouThr=.75)\n",
    "            stats[3] = _summarize(1, maxDets=20, areaRng='medium')\n",
    "            stats[4] = _summarize(1, maxDets=20, areaRng='large')\n",
    "            stats[5] = _summarize(0, maxDets=20)\n",
    "            stats[6] = _summarize(0, maxDets=20, iouThr=.5)\n",
    "            stats[7] = _summarize(0, maxDets=20, iouThr=.75)\n",
    "            stats[8] = _summarize(0, maxDets=20, areaRng='medium')\n",
    "            stats[9] = _summarize(0, maxDets=20, areaRng='large')\n",
    "            return stats\n",
    "        if not self.eval:\n",
    "            raise Exception('Please run accumulate() first')\n",
    "        iouType = self.params.iouType\n",
    "        if iouType == 'segm' or iouType == 'bbox':\n",
    "            summarize = _summarizeDets\n",
    "        elif iouType == 'keypoints':\n",
    "            summarize = _summarizeKps\n",
    "        self.stats = summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad92a259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_COCOEvaluator(COCOEvaluator):\n",
    "    def _evaluate_predictions_on_coco(\n",
    "        coco_gt, coco_results, iou_type, kpt_oks_sigmas=None, use_fast_impl=False, img_ids=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Evaluate the coco results using COCOEval API.\n",
    "        \"\"\"\n",
    "        assert len(coco_results) > 0\n",
    "\n",
    "        if iou_type == \"segm\":\n",
    "            coco_results = copy.deepcopy(coco_results)\n",
    "            # When evaluating mask AP, if the results contain bbox, cocoapi will\n",
    "            # use the box area as the area of the instance, instead of the mask area.\n",
    "            # This leads to a different definition of small/medium/large.\n",
    "            # We remove the bbox field to let mask AP use mask area.\n",
    "            for c in coco_results:\n",
    "                c.pop(\"bbox\", None)\n",
    "\n",
    "        coco_dt = coco_gt.loadRes(coco_results)\n",
    "        coco_eval = (COCOeval_opt if use_fast_impl else My_COCOeval)(coco_gt, coco_dt, iou_type)\n",
    "        if img_ids is not None:\n",
    "            coco_eval.params.imgIds = img_ids\n",
    "\n",
    "        if iou_type == \"keypoints\":\n",
    "            # Use the COCO default keypoint OKS sigmas unless overrides are specified\n",
    "            if kpt_oks_sigmas:\n",
    "                assert hasattr(coco_eval.params, \"kpt_oks_sigmas\"), \"pycocotools is too old!\"\n",
    "                coco_eval.params.kpt_oks_sigmas = np.array(kpt_oks_sigmas)\n",
    "            # COCOAPI requires every detection and every gt to have keypoints, so\n",
    "            # we just take the first entry from both\n",
    "            num_keypoints_dt = len(coco_results[0][\"keypoints\"]) // 3\n",
    "            num_keypoints_gt = len(next(iter(coco_gt.anns.values()))[\"keypoints\"]) // 3\n",
    "            num_keypoints_oks = len(coco_eval.params.kpt_oks_sigmas)\n",
    "            assert num_keypoints_oks == num_keypoints_dt == num_keypoints_gt, (\n",
    "                f\"[COCOEvaluator] Prediction contain {num_keypoints_dt} keypoints. \"\n",
    "                f\"Ground truth contains {num_keypoints_gt} keypoints. \"\n",
    "                f\"The length of cfg.TEST.KEYPOINT_OKS_SIGMAS is {num_keypoints_oks}. \"\n",
    "                \"They have to agree with each other. For meaning of OKS, please refer to \"\n",
    "                \"http://cocodataset.org/#keypoints-eval.\"\n",
    "            )\n",
    "\n",
    "        coco_eval.evaluate()\n",
    "        coco_eval.accumulate()\n",
    "        coco_eval.summarize()\n",
    "\n",
    "        return coco_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfb6011a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Dataset 'ww_train' is already registered!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mregister_coco_instances\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mww_train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/ww_train.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m register_coco_instances(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mww_eval\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/ww_val.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m register_coco_instances(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mww_test\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/ww_test.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/yandex/MLW2022/idoc/detectron2/detectron2/detectron2/data/datasets/register_coco.py:37\u001b[0m, in \u001b[0;36mregister_coco_instances\u001b[0;34m(name, metadata, json_file, image_root)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image_root, (\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike)), image_root\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# 1. register a function which returns dicts\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[43mDatasetCatalog\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_coco_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# 2. Optionally, add metadata about this dataset,\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# since they might be useful in evaluation, visualization or logging\u001b[39;00m\n\u001b[1;32m     41\u001b[0m MetadataCatalog\u001b[38;5;241m.\u001b[39mget(name)\u001b[38;5;241m.\u001b[39mset(\n\u001b[1;32m     42\u001b[0m     json_file\u001b[38;5;241m=\u001b[39mjson_file, image_root\u001b[38;5;241m=\u001b[39mimage_root, evaluator_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoco\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetadata\n\u001b[1;32m     43\u001b[0m )\n",
      "File \u001b[0;32m/home/yandex/MLW2022/idoc/detectron2/detectron2/detectron2/data/catalog.py:37\u001b[0m, in \u001b[0;36m_DatasetCatalog.register\u001b[0;34m(self, name, func)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    name (str): the name that identifies a dataset, e.g. \"coco_2014_train\".\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    func (callable): a callable which takes no arguments and returns a list of dicts.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m        It must return the same results if called multiple times.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m callable(func), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must register a function with `DatasetCatalog.register`!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is already registered!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m[name] \u001b[38;5;241m=\u001b[39m func\n",
      "\u001b[0;31mAssertionError\u001b[0m: Dataset 'ww_train' is already registered!"
     ]
    }
   ],
   "source": [
    "register_coco_instances(\"ww_train\", {}, \"../data/ww_train.json\", \"../data/\")\n",
    "register_coco_instances(\"ww_eval\", {}, \"../data/ww_val.json\", \"../data/\")\n",
    "register_coco_instances(\"ww_test\", {}, \"../data/ww_test.json\", \"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "148dd017",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ROI_HEADS_REGISTRY.register()\n",
    "class My_Res5ROIHeads(Res5ROIHeads):\n",
    "    # def __init__(\n",
    "    #         self,\n",
    "    #         *,\n",
    "    #         in_features: List[str],\n",
    "    #         pooler: ROIPooler,\n",
    "    #         res5: nn.Module,\n",
    "    #         box_predictor: nn.Module,\n",
    "    #         mask_head: Optional[nn.Module] = None,\n",
    "    #         **kwargs,\n",
    "    # ):\n",
    "    #     super().__init__(in_features,\n",
    "    #                      pooler,\n",
    "    #                      res5,\n",
    "    #                      box_predictor,\n",
    "    #                      mask_head, kwargs)\n",
    "    def forward(\n",
    "            self,\n",
    "            images,\n",
    "            features,\n",
    "            proposals,\n",
    "            targets=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        See :meth:`ROIHeads.forward`.\n",
    "        \"\"\"\n",
    "        del images\n",
    "\n",
    "        if self.training:\n",
    "            assert targets\n",
    "            proposals = self.label_and_sample_proposals(proposals, targets)\n",
    "        del targets\n",
    "\n",
    "        proposal_boxes = [x.proposal_boxes for x in proposals]\n",
    "        box_features = self._shared_roi_transform(\n",
    "            [features[f] for f in self.in_features], proposal_boxes\n",
    "        )\n",
    "        predictions = self.box_predictor(box_features.mean(dim=[2, 3]))\n",
    "\n",
    "        if self.training:\n",
    "            del features\n",
    "            losses = self.box_predictor.losses(predictions, proposals)\n",
    "            if self.mask_on:\n",
    "                proposals, fg_selection_masks = select_foreground_proposals(\n",
    "                    proposals, self.num_classes\n",
    "                )\n",
    "                # Since the ROI feature transform is shared between boxes and masks,\n",
    "                # we don't need to recompute features. The mask loss is only defined\n",
    "                # on foreground proposals, so we need to select out the foreground\n",
    "                # features.\n",
    "                mask_features = box_features[torch.cat(fg_selection_masks, dim=0)]\n",
    "                del box_features\n",
    "                losses.update(self.mask_head(mask_features, proposals))\n",
    "            return [], losses\n",
    "        else:\n",
    "            pred_instances, _ = self.box_predictor.inference(predictions, proposals)\n",
    "            pred_instances = self.forward_with_given_boxes(features, pred_instances)\n",
    "        # for editing the proposals boxes  proposals[0]._fields[\"proposal_boxes\"].tensor\n",
    "        orig_res = pred_instances, {}\n",
    "        # orig_res = super().forward(images, features, proposals, targets)\n",
    "        if not self.training:\n",
    "            # print(\"=\" * 9 + \"my module\" + \"=\" * 9)\n",
    "            if self.filter_flag:\n",
    "                ins = pred_instances[0]\n",
    "                # ins[ins._fields[\"pred_classes\"] == 2]._fields[\"pred_boxes\"]\n",
    "                keep = torch.ones(len(ins), dtype=bool,device=ins._fields[\"pred_classes\"].device)\n",
    "                iou_mat = detectron2.structures.pairwise_iou(\n",
    "                    ins[ins._fields[\"pred_classes\"] == self.OPEN_FLOCK]._fields[\"pred_boxes\"],\n",
    "                    ins[ins._fields[\"pred_classes\"] == self.SPHERICAL_FLOCK]._fields[\"pred_boxes\"])\n",
    "                keep[ins._fields[\"pred_classes\"] == self.OPEN_FLOCK] = iou_mat.max(dim=1).values > self.filter_threshold\n",
    "                ins._fields[\"pred_classes\"] = ins._fields[\"pred_classes\"][keep]\n",
    "                ins._fields[\"scores\"] = ins._fields[\"scores\"][keep]\n",
    "                ins._fields[\"pred_boxes\"] = ins._fields[\"pred_boxes\"][keep]\n",
    "                # detectron2.structures.boxes.pairwise_intersection(\n",
    "                #     ins[ins._fields[\"pred_classes\"] == 2]._fields[\"pred_boxes\"],\n",
    "                #     ins[ins._fields[\"pred_classes\"] == 2]._fields[\"pred_boxes\"])\n",
    "            # get the predicted boxes with pred_instances[0]._fields[\"pred_boxes\"].tensor\n",
    "            # get the class predicted for that box with pred_instances[0]._fields[\"pred_classes\"]\n",
    "            # todo need to make sure that removing a sample from the prediction don't break anything\n",
    "\n",
    "            # print(orig_res)\n",
    "            # print(\"=\" * 22)\n",
    "        return orig_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b522f3ea",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da540e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_C4_1x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"ww_train\",)\n",
    "cfg.DATASETS.TEST = (\"ww_eval\",)  # no metrics implemented for this dataset\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_C4_1x.yaml\")  # initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.002\n",
    "cfg.SOLVER.MAX_ITER = 1000  # 300 iterations seems good enough, but you can certainly train longer\n",
    "# cfg.MODEL.ROI_HEADS.NAME = \"My_Res5ROIHeads\"\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128  # faster, and good enough for this toy dataset\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 5  # 3 classes (data, fig, hazelnut)\n",
    "cfg.TEST.EVAL_PERIOD = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b4e796",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c04338",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.model.roi_heads.filter_threshold = 0.13\n",
    "trainer.model.roi_heads.SPHERICAL_FLOCK = 4\n",
    "trainer.model.roi_heads.OPEN_FLOCK = 2\n",
    "trainer.model.roi_heads.filter_flag = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8866f5",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08b8651b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[08/13 12:47:26 d2.modeling.backbone.resnet]: \u001b[0mResNet.make_stage(first_stride=) is deprecated!  Use 'stride_per_block' or 'stride' instead.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[08/13 12:47:27 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[08/13 12:47:27 d2.data.datasets.coco]: \u001b[0mLoaded 264 images in COCO format from ../data/ww_test.json\n",
      "\u001b[32m[08/13 12:47:27 d2.data.common]: \u001b[0mSerializing 264 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[08/13 12:47:27 d2.data.common]: \u001b[0mSerialized dataset takes 0.06 MiB\n",
      "\u001b[32m[08/13 12:47:27 d2.data.dataset_mapper]: \u001b[0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[08/13 12:47:27 d2.evaluation.evaluator]: \u001b[0mStart inference on 264 images\n",
      "\u001b[32m[08/13 12:47:28 d2.evaluation.evaluator]: \u001b[0mInference done 11/264. 0.0879 s / img. ETA=0:00:29\n",
      "\u001b[32m[08/13 12:47:33 d2.evaluation.evaluator]: \u001b[0mInference done 54/264. 0.0910 s / img. ETA=0:00:24\n",
      "\u001b[32m[08/13 12:47:38 d2.evaluation.evaluator]: \u001b[0mInference done 98/264. 0.0937 s / img. ETA=0:00:19\n",
      "\u001b[32m[08/13 12:47:43 d2.evaluation.evaluator]: \u001b[0mInference done 144/264. 0.0964 s / img. ETA=0:00:13\n",
      "\u001b[32m[08/13 12:47:48 d2.evaluation.evaluator]: \u001b[0mInference done 188/264. 0.0984 s / img. ETA=0:00:08\n",
      "\u001b[32m[08/13 12:47:53 d2.evaluation.evaluator]: \u001b[0mInference done 230/264. 0.0994 s / img. ETA=0:00:03\n",
      "\u001b[32m[08/13 12:47:58 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:30.073095 (0.116112 s / img per device, on 1 devices)\n",
      "\u001b[32m[08/13 12:47:58 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:25 (0.100088 s / img per device, on 1 devices)\n",
      "\u001b[32m[08/13 12:47:58 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[08/13 12:47:58 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ../output1/coco_instances_results.json\n",
      "\u001b[32m[08/13 12:47:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "COCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "Accumulating evaluation results...\n",
      "COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.083\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.242\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.035\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.083\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.215\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.311\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.311\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.311\n",
      "\u001b[32m[08/13 12:47:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 8.260 | 24.228 | 3.515  |  nan  |  nan  | 8.303 |\n",
      "\u001b[32m[08/13 12:47:58 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[08/13 12:47:58 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP    | category       | AP     | category   | AP    |\n",
      "|:-----------|:------|:---------------|:-------|:-----------|:------|\n",
      "| Micro      | nan   | filament       | 0.505  | open_floc  | 6.631 |\n",
      "| protozoa   | 6.850 | spherical_floc | 19.054 |            |       |\n"
     ]
    }
   ],
   "source": [
    "model = build_model(cfg)\n",
    "DetectionCheckpointer(model).load(\"output/model_final.pth\")\n",
    "test_loader = build_detection_test_loader(cfg, \"ww_test\")\n",
    "evaluator = My_COCOEvaluator(\"ww_test\", cfg, False, output_dir=\"../output1/\")\n",
    "out = inference_on_dataset(model, test_loader, evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0061aa6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('bbox',\n",
       "              {'AP': 8.259838370008726,\n",
       "               'AP50': 24.228138871452074,\n",
       "               'AP75': 3.5146747825044558,\n",
       "               'APs': nan,\n",
       "               'APm': nan,\n",
       "               'APl': 8.30319171057265,\n",
       "               'AP-Micro': nan,\n",
       "               'AP-filament': 0.5049504950495048,\n",
       "               'AP-open_floc': 6.6306046206946485,\n",
       "               'AP-protozoa': 6.849856032187854,\n",
       "               'AP-spherical_floc': 19.0539423321029})])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05115990",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
